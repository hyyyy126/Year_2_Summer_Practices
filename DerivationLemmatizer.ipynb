{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88127e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import Levenshtein\n",
    "\n",
    "\"\"\"\n",
    "This program uses a \"length method\" in an attempt to lemmatize derivational changes of words, such as \"apply\" and \"application\".\n",
    "In traditional NLP lemmatizers like NLTK and SpaCy, they only focuses on inflectional changes like \"-s\" or \"-ed\",\n",
    "which are only temporary grammar changes.\n",
    "However, if we want to consider \"apply\" and derived \"application\" as the same word since they share the same origin and being related in meaning,\n",
    "and only differed in the Part of Speech, we may need to discover other alternative methods.\n",
    "For the mass users, derivational lemmatizations can help them to acquire a wider range of related lexical information simutaneously,\n",
    "which can be convenient during information searching.\n",
    "In this program, an alternative method considering different word part legnth parameters and word formation structures is introduced.\n",
    "For the prefixes, they are usually meaning carrying like \"un-\" or \"dis-\"'s negation.\n",
    "Hence, it is still not preferable to overlemmatize \"unhappy\" as \"happy\" because this will cause inconvenience in real applications.\n",
    "\"\"\"\n",
    "\n",
    "# Define a target list for desired derivational lemmas (personalized)\n",
    "target = {\"apply\"}\n",
    "# Use specific, large corpora can perform more accurate in finding the most suitable candidate\n",
    "freqlist = nltk.FreqDist(nltk.corpus.brown.words())\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The basic SpaCy lemmatizer handling inflectional changes\n",
    "def inf_lemmatize(word: str) -> str:\n",
    "\n",
    "    word = word.strip().lower()\n",
    "    doc = nlp(word)\n",
    "\n",
    "    for token in doc:\n",
    "        return token.lemma_\n",
    "\n",
    "def my_lemmatizer(word: str) -> str:\n",
    "\n",
    "    # Check word string validity\n",
    "    if not word:\n",
    "        return None\n",
    "    \n",
    "    # Take the last word for lemmatization\n",
    "    word = word.split()[-1]\n",
    "    word = word.strip().lower()\n",
    "\n",
    "    # Use SpaCy to inflectionally lemmatize first\n",
    "    spacy_lemma = inf_lemmatize(word)\n",
    "    if spacy_lemma in target:\n",
    "        return spacy_lemma\n",
    "    \n",
    "    # According to the BNC retrieved from Sketch Engine,\n",
    "    # the average length of top 1000 frequent noun, verb and adjective is 6.403.\n",
    "    # Which 3000 words have already included affixes,\n",
    "    # and it means that the \"lemma root\" will be certainly shorter than 6 in length.\n",
    "    # Therefore, an average length of roots 3 (6/2) is tried to apply considering to short words,\n",
    "    # and len(word)//2 alternative is further availiable for long words to better balance on not being overlemmatized.\n",
    "    average_root_length = max(3, len(word)//2)\n",
    "\n",
    "    suffix_part = len(word)-average_root_length\n",
    "\n",
    "    for candidate in freqlist:\n",
    "\n",
    "        candidate = candidate.lower()\n",
    "\n",
    "        # Keeping the \"non-suffix\" part to check how slimiar are the word and lemma candidate\n",
    "        sliced = candidate[:average_root_length]\n",
    "\n",
    "        # \"Underived\" lemma should usually be shorter than the the derivant\n",
    "        # When a word starts with the \"sliced\" part, they have the potential to be the deriavation pair in further checking\n",
    "        # (like \"apply\" and \"application\" are both start with \"app\")\n",
    "        # The small Levenshtein distance ensures the pair are only differed for the suffixes\n",
    "        # Levenshtein distance is chosen because of its flexibility (+1 for both add, delete and replace)\n",
    "        if len(candidate)<len(word) and \\\n",
    "            word.startswith(sliced) and \\\n",
    "            Levenshtein.distance(word, candidate)<=suffix_part:\n",
    "                \n",
    "                return candidate\n",
    "\n",
    "# Continously find the lemma in the target.\n",
    "# application -> applied -> apply\n",
    "# Personalized list are perfered to adjust different strictness (apply -> app may not be a desired result)\n",
    "def my_lemmatizer_recursion(word: str) -> str:\n",
    "    lemma = my_lemmatizer(word)\n",
    "\n",
    "    if not lemma:\n",
    "        return None\n",
    "    \n",
    "    if lemma in target:\n",
    "        return lemma\n",
    "    \n",
    "    return my_lemmatizer_recursion(lemma)\n",
    "\n",
    "print(my_lemmatizer_recursion(\"application\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
